# -*- coding: utf-8 -*-
import os
import json
import torch
import re
import dashscope
from dashscope import Generation
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from tqdm import tqdm

# ===========================
# 1. é…ç½®åŒºåŸŸ
# ===========================
dashscope.api_key = os.getenv("DASHSCOPE_API_KEY") or "sk-7ba2fb93588a45a3b5b120658f45dc79"  

BASE_MODEL_PATH = "./Qwen1.5-1.8B"  # åŸºåº§è·¯å¾„
LORA_MODEL_PATH = "model_result/qwen_lora_sft"  # LoRA è·¯å¾„
TEST_DATA = "dataset/final_dataset.jsonl"  # æ•°æ®è·¯å¾„

# æµ‹è¯•æ ·æœ¬æ•°
TEST_COUNT = 3


# ===========================
# 2. å·¥å…·å‡½æ•°
# ===========================
def extract_json(text):
    """ä»å¤§æ¨¡å‹è¾“å‡ºä¸­æå– JSON"""
    try:
        # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª }
        match = re.search(r'\{.*\}', text.replace("\n", ""), re.DOTALL)
        if match:
            return json.loads(match.group(0))
        return json.loads(text)
    except:
        return {"base_score": 0, "lora_score": 0, "reason": "æ‰“åˆ†è§£æå¤±è´¥"}


def llm_compare_judge(question, truth, base_ans, lora_ans):
    """è®©è£åˆ¤å¯¹æ¯”ä¸¤ä¸ªå›ç­”å¹¶æ‰“åˆ†"""
    prompt = f"""
    ä½ æ˜¯ä¸€ä½æ³•å¾‹ä¸“å®¶ã€‚è¯·æ ¹æ®ã€æ ‡å‡†ç­”æ¡ˆã€‘ï¼Œå¯¹æ¯”è¯„ä»·ã€å›ç­”Aã€‘(åŸºåº§æ¨¡å‹)å’Œã€å›ç­”Bã€‘(å¾®è°ƒæ¨¡å‹)ã€‚

    ã€é—®é¢˜ã€‘ï¼š{question}
    ã€æ ‡å‡†ç­”æ¡ˆã€‘ï¼š{truth}

    ã€å›ç­”Aã€‘ï¼š{base_ans}
    ã€å›ç­”Bã€‘ï¼š{lora_ans}

    è¯·åˆ†åˆ«æ‰“åˆ†(0-10åˆ†)ï¼Œå¹¶ç»™å‡ºç®€çŸ­ç‚¹è¯„ã€‚
    å¿…é¡»è¿”å›åˆæ³•çš„ JSON æ ¼å¼ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
    {{
        "base_score": <æ•°å­—>,
        "lora_score": <æ•°å­—>,
        "reason": "<50å­—ä»¥å†…çš„å¯¹æ¯”ç‚¹è¯„>"
    }}
    """
    try:
        resp = Generation.call(model="qwen-plus", prompt=prompt, result_format='message')
        if resp.status_code == 200:
            return extract_json(resp.output.choices[0].message.content)
    except Exception as e:
        print(f"Judge Error: {e}")
    return {"base_score": 0, "lora_score": 0, "reason": "API è°ƒç”¨å¤±è´¥"}


def generate(model, tokenizer, text):
    """ç»Ÿä¸€ç”Ÿæˆé€»è¾‘"""
    messages = [{"role": "user", "content": text}]
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([input_text], return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            do_sample=False
        )
    return tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip()


# ===========================
# 3. ä¸»ç¨‹åº
# ===========================
def main():
    # --- å‡†å¤‡æ•°æ® ---
    questions = []
    with open(TEST_DATA, "r", encoding="utf-8") as f:
        for line in f:
            questions.append(json.loads(line))
    questions = questions[:TEST_COUNT]  # åªå–å‰å‡ æ¡

    # --- é˜¶æ®µ 1: åŸºåº§æ¨¡å‹æ¨ç† ---
    print(f">>> 1/3 æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹ ({BASE_MODEL_PATH})...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH, device_map="auto", torch_dtype=torch.float16, trust_remote_code=True
    )

    print(">>> åŸºåº§æ¨¡å‹æ­£åœ¨ä½œç­”...")
    base_answers = []
    for q in tqdm(questions):
        prompt = f"{q['instruction']}\n{q.get('input', '')}" if q.get('input') else q['instruction']
        base_answers.append(generate(model, tokenizer, prompt))

    # --- é˜¶æ®µ 2: LoRA æ¨¡å‹æ¨ç† ---
    print(f"\n>>> 2/3 æ­£åœ¨æŒ‚è½½ LoRA æƒé‡ ({LORA_MODEL_PATH})...")
    model = PeftModel.from_pretrained(model, LORA_MODEL_PATH)

    print(">>> å¾®è°ƒæ¨¡å‹æ­£åœ¨ä½œç­”...")
    lora_answers = []
    for q in tqdm(questions):
        prompt = f"{q['instruction']}\n{q.get('input', '')}" if q.get('input') else q['instruction']
        lora_answers.append(generate(model, tokenizer, prompt))

    # --- é˜¶æ®µ 3: è£åˆ¤æ‰“åˆ†ä¸å±•ç¤º ---
    print("\n>>> 3/3 æ­£åœ¨è¯·æ±‚å¤§æ¨¡å‹è£åˆ¤æ‰“åˆ†å¹¶ç”ŸæˆæŠ¥å‘Š...\n")

    total_base_score = 0
    total_lora_score = 0

    results = []

    # é€æ¡æ‰“åˆ†
    for i in range(len(questions)):
        res = llm_compare_judge(
            questions[i]["instruction"],
            questions[i]["output"],
            base_answers[i],
            lora_answers[i]
        )
        results.append(res)
        total_base_score += res["base_score"]
        total_lora_score += res["lora_score"]

    # ===========================
    # æœ€ç»ˆæŠ¥å‘Šè¾“å‡º
    # ===========================
    print("â–ˆ" * 60)
    print("               å¾®è°ƒæ•ˆæœè¯„ä¼°æŠ¥å‘Š               ")
    print("â–ˆ" * 60 + "\n")

    # 1. æ€»ä½“å¹³å‡åˆ†
    avg_base = total_base_score / TEST_COUNT
    avg_lora = total_lora_score / TEST_COUNT

    print(f"ğŸ“Š ã€æ€»ä½“è¯„åˆ†ã€‘ (å…± {TEST_COUNT} ä¸ªæµ‹è¯•æ¡ˆä¾‹)")
    print(f"----------------------------------------")
    print(f"ğŸ”´ åŸºåº§æ¨¡å‹ (Base):  {avg_base:.1f} / 10.0")
    print(f"ğŸŸ¢ å¾®è°ƒæ¨¡å‹ (LoRA):  {avg_lora:.1f} / 10.0")
    print(f"ğŸ“ˆ æå‡å¹…åº¦:        +{avg_lora - avg_base:.1f} åˆ†")
    print(f"----------------------------------------\n")

    # 2. å…·ä½“æ¡ˆä¾‹å±•ç¤º
    print(f"ğŸ“ ã€å…·ä½“æ¡ˆä¾‹è¯¦æƒ…ã€‘")
    for i in range(TEST_COUNT):
        q = questions[i]
        r = results[i]

        print(f"æ¡ˆä¾‹ #{i + 1}")
        print(f"é—®ï¼š{q['instruction']}")
        print("-" * 50)
        print(f"ğŸ”´ åŸºåº§å›ç­” ({r['base_score']}åˆ†):")
        print(f"{base_answers[i][:100]}..." if len(base_answers[i]) > 100 else base_answers[i])  # åªæ˜¾ç¤ºå‰100å­—é¿å…åˆ·å±
        print("-" * 50)
        print(f"ğŸŸ¢ å¾®è°ƒå›ç­” ({r['lora_score']}åˆ†):")
        print(f"{lora_answers[i][:100]}..." if len(lora_answers[i]) > 100 else lora_answers[i])
        print("-" * 50)
        print(f"ğŸ’¡ ä¸“å®¶ç‚¹è¯„: {r['reason']}")
        print("=" * 60 + "\n")


if __name__ == "__main__":
    main()
