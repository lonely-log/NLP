import torch
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

BASE_MODEL = "./Qwen1.5-1.8B"
LORA_PATH = "model_result/qwen_lora_sft_3h/checkpoint-8000"


# ===========================
# åŠ è½½æ¨¡å‹ (Base + LoRA)
# ===========================
def load_model():
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)

    print("Loading base model...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )

    print("Applying LoRA weights...")
    model = PeftModel.from_pretrained(
        base_model,
        LORA_PATH,
        device_map="auto"
    )

    model.eval()
    print("Model loaded!")
    return tokenizer, model


tokenizer, model = load_model()


# ===========================
# æ¨ç†å‡½æ•°ï¼ˆå¤šè½®å¯¹è¯ï¼‰
# ===========================
def predict(history, user_input):
    """
    Gradio history: List[List[str, str]]
    Our format:     [(user, assistant), ...]
    """
    # æ„å»º promptï¼ˆæ‹¼æ¥å†å²ï¼‰
    prompt = ""
    for user, assistant in history:
        prompt += f"ç”¨æˆ·ï¼š{user}\nåŠ©æ‰‹ï¼š{assistant}\n"

    prompt += f"ç”¨æˆ·ï¼š{user_input}\nåŠ©æ‰‹ï¼š"

    # ç¼–ç 
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # æ¨ç†
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.05
        )

    # è§£ç ç»“æœ
    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # æˆªå– assistant éƒ¨åˆ†
    answer = full_output[len(prompt):].strip()

    # æ›´æ–° history
    history.append((user_input, answer))
    return history, history


# ===========================
# Gradio UI
# ===========================
def clear_history():
    return [], []


with gr.Blocks(title="LoRA-Qwen Chat UI") as demo:
    gr.Markdown("<h2><center>ğŸ§  LoRA å¾®è°ƒ Qwen å¯¹è¯ç•Œé¢</center></h2>")

    chatbot = gr.Chatbot(height=500)
    user_input = gr.Textbox(
        label="ä½ çš„è¾“å…¥",
        placeholder="è¯·è¾“å…¥ä½ çš„é—®é¢˜â€¦",
    )

    submit_btn = gr.Button("å‘é€")
    clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯")

    submit_btn.click(
        predict,
        inputs=[chatbot, user_input],
        outputs=[chatbot, chatbot]
    )

    clear_btn.click(clear_history, outputs=[chatbot, chatbot])


# ===========================
# å¯åŠ¨æœåŠ¡
# ===========================
if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
